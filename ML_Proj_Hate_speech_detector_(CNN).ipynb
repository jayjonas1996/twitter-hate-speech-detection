{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jayjonas1996/twitter-hate-speech-detection/blob/main/ML_Proj_Hate_speech_detector_(CNN).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcUsmnjnlIsV",
        "outputId": "3eb7ee90-1b7e-4498-8fd9-ba60ddfb2d21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting keras==1.2.2\n",
            "  Downloading Keras-1.2.2.tar.gz (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 4.8 MB/s \n",
            "\u001b[?25hCollecting theano\n",
            "  Downloading Theano-1.0.5.tar.gz (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 35.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==1.2.2) (3.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from keras==1.2.2) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from theano->keras==1.2.2) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from theano->keras==1.2.2) (1.4.1)\n",
            "Building wheels for collected packages: keras, theano\n",
            "  Building wheel for keras (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras: filename=Keras-1.2.2-py3-none-any.whl size=209601 sha256=c54cbd8358bef8869d39d3d51742af8b187bcb166f0fa1c3de03fd677a895b76\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/32/23/2a1db3765ec19c91503843380a4f92b6530598949c661c5fa2\n",
            "  Building wheel for theano (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for theano: filename=Theano-1.0.5-py3-none-any.whl size=2668111 sha256=49ae7a18eab9068ca0812f57b22c8fc2ec116a173ba6c00cc45773f431c09967\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/68/6f/745330367ce7822fe0cd863712858151f5723a0a5e322cc144\n",
            "Successfully built keras theano\n",
            "Installing collected packages: theano, keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "tensorflow 2.8.0 requires keras<2.9,>=2.8.0rc0, but you have keras 1.2.2 which is incompatible.\n",
            "Successfully installed keras-1.2.2 theano-1.0.5\n",
            "Collecting tensorflow==1.15.2\n",
            "  Downloading tensorflow-1.15.2-cp37-cp37m-manylinux2010_x86_64.whl (110.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5 MB 35 kB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.14.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.21.6)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.1.2)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.3.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.8 MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 48.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.0.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.37.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.44.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.8.1)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 34.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.17.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.2.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (4.2.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15.2) (1.5.2)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=a23100a25342969a0c09c6cba955adbe0619d6439550d6b345eb7a8c99428af9\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.0\n",
            "    Uninstalling tensorflow-2.8.0:\n",
            "      Successfully uninstalled tensorflow-2.8.0\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.2 which is incompatible.\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.2 tensorflow-estimator-1.15.1\n"
          ]
        }
      ],
      "source": [
        "# Pinning the important libraries to a specific version\n",
        "!pip install keras==1.2.2 --no-color\n",
        "!pip install tensorflow==1.15.2 --no-color"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZKoMaiRm2Uu"
      },
      "source": [
        "Importing the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g66wKmAhhXuo",
        "outputId": "8463d7cc-1055-44f5-c356-bfc8ac3c1cbb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import re\n",
        "from string import punctuation\n",
        "from collections import defaultdict\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gensim\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "\n",
        "import keras\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import np_utils\n",
        "from keras.layers import Embedding, Input, LSTM\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Activation, Dense, Dropout, Embedding, \\\n",
        "                         Input, Convolution1D, MaxPooling1D, \\\n",
        "                         GlobalMaxPooling1D, Merge, InputLayer\n",
        "\n",
        "import sklearn\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import make_scorer, f1_score, accuracy_score, recall_score, \\\n",
        "                            precision_score, classification_report, \\\n",
        "                            precision_recall_fscore_support\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjPsEYWRlvcn"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_DIM = 25\n",
        "LOSS_FUN = \"categorical_crossentropy\"\n",
        "OPTIMIZER = \"adam\"\n",
        "NO_OF_FOLDS = 5\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 128\n",
        "FLAGS = re.MULTILINE | re.DOTALL\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXTA5Zmwm0Pm"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uU0UXwSSh1Em",
        "outputId": "73490afb-808e-4aa6-d292-adcbbded519f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-70ba6ef8-d3d1-44e0-ac72-17e86408976a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>@user when a father is dysfunctional and is s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>bihday your majesty</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>#model   i love u take with u all the time in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>factsguide: society now    #motivation</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-70ba6ef8-d3d1-44e0-ac72-17e86408976a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-70ba6ef8-d3d1-44e0-ac72-17e86408976a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-70ba6ef8-d3d1-44e0-ac72-17e86408976a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   label                                              tweet\n",
              "0      0   @user when a father is dysfunctional and is s...\n",
              "1      0  @user @user thanks for #lyft credit i can't us...\n",
              "2      0                                bihday your majesty\n",
              "3      0  #model   i love u take with u all the time in ...\n",
              "4      0             factsguide: society now    #motivation"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import the dataset\n",
        "\n",
        "GLOVE_MODEL_FILE = \"/content/gdrive/My Drive/Dataset/glove.twitter.27B.25d.txt\"\n",
        "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(GLOVE_MODEL_FILE)\n",
        "\n",
        "df = pd.read_csv('/content/gdrive/My Drive/Dataset/twitter_hate_speech_data.csv')\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-6iplm4ftK2"
      },
      "outputs": [],
      "source": [
        "def hashtag(text):\n",
        "    text = text.group()\n",
        "    hashtag_body = text[1:]\n",
        "    if hashtag_body.isupper():\n",
        "        result = u\"<hashtag> {} <allcaps>\".format(hashtag_body)\n",
        "    else:\n",
        "        result = \" \".join([\"<hashtag>\"] + re.split(r'(?=[A-Z])', hashtag_body))\n",
        "    return result\n",
        "\n",
        "def allcaps(text):\n",
        "    text = text.group()\n",
        "    return text.lower() + \" <allcaps>\"\n",
        "\n",
        "\n",
        "def re_sub(text, pattern, repl):\n",
        "    return re.sub(pattern, repl, text, flags=FLAGS)\n",
        "\n",
        "def tokenizer_g(text):\n",
        "    # Different regex parts for smiley faces\n",
        "    eyes = r\"[8:=;]\"\n",
        "    nose = r\"['`\\-]?\"\n",
        "\n",
        "    text = re_sub(text, r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"<url>\")\n",
        "    text = re_sub(text, r\"/\",\" / \")\n",
        "    text = re_sub(text, r\"@\\w+\", \"<user>\")\n",
        "    text = re_sub(text, r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes), \"<smile>\")\n",
        "    text = re_sub(text, r\"{}{}p+\".format(eyes, nose), \"<lolface>\")\n",
        "    text = re_sub(text, r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \"<sadface>\")\n",
        "    text = re_sub(text, r\"{}{}[\\/|l*]\".format(eyes, nose), \"<neutralface>\")\n",
        "    text = re_sub(text, r\"<3\",\"<heart>\")\n",
        "    text = re_sub(text, r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<number>\")\n",
        "    text = re_sub(text, r\"#\\S+\", hashtag)\n",
        "    text = re_sub(text, r\"([!?.]){2,}\", r\"\\1 <repeat>\")\n",
        "    text = re_sub(text, r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 <elong>\")\n",
        "\n",
        "    text = re_sub(text, r\"([A-Z]){2,}\", allcaps)\n",
        "\n",
        "    return text.lower()\n",
        "\n",
        "def glove_tokenize(text):\n",
        "    text = tokenizer_g(text)\n",
        "    text = ''.join([c for c in text if c not in punctuation])\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in STOPWORDS]\n",
        "    return words\n",
        "\n",
        "\n",
        "def gen_vocab(tweets_df):\n",
        "    vocab, reverse_vocab = {}, {}\n",
        "    freq = defaultdict(int)\n",
        "    vocab_index = 1\n",
        "    for index, row in tweets_df.iterrows():\n",
        "        words = glove_tokenize(row['tweet'].lower())\n",
        "\n",
        "        for word in words:\n",
        "            if word not in vocab:\n",
        "                vocab[word] = vocab_index\n",
        "                reverse_vocab[vocab_index] = word       # generate reverse vocab as well\n",
        "                vocab_index += 1\n",
        "            freq[word] += 1\n",
        "    vocab['UNK'] = len(vocab) + 1 # count unknown words\n",
        "    reverse_vocab[len(vocab)] = 'UNK'\n",
        "    return vocab, reverse_vocab, freq\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbwG_bWFiejg"
      },
      "outputs": [],
      "source": [
        "vocab, reverse_vocab = {}, {}\n",
        "freq = defaultdict(int)\n",
        "vocab_index = 1\n",
        "\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    _emb = 0\n",
        "    words = glove_tokenize(row['tweet'].lower())\n",
        "\n",
        "    for word in words:\n",
        "        # Check if embeeding there in GLove model\n",
        "        if word in word2vec_model:\n",
        "            _emb += 1\n",
        "\n",
        "        # Drop the rows where no word is found in glove after cleanup \n",
        "        # because of empty or useless small tweet \n",
        "        if not _emb and index in df.index:\n",
        "          df.drop(index, inplace=True, axis=0)\n",
        "\n",
        "        # Add words to new vocab\n",
        "        if word not in vocab:\n",
        "            vocab[word] = vocab_index\n",
        "            reverse_vocab[vocab_index] = word\n",
        "            vocab_index += 1\n",
        "        freq[word] += 1\n",
        "          \n",
        "    # count unknown words\n",
        "    vocab['UNK'] = len(vocab) + 1\n",
        "    reverse_vocab[len(vocab)] = 'UNK'\n",
        "\n",
        "def shuffle_weights(model):\n",
        "    weights = model.get_weights()\n",
        "    weights = [np.random.permutation(w.flat).reshape(w.shape) for w in weights]\n",
        "    model.set_weights(weights)\n",
        "\n",
        "def batch_gen(X, batch_size):\n",
        "    import math\n",
        "    n_batches = int(math.ceil(X.shape[0] / float(batch_size)))\n",
        "    end = int(X.shape[0] / float(batch_size)) * batch_size\n",
        "    n = 0\n",
        "    for i in range(0, n_batches):\n",
        "        if i < n_batches - 1: \n",
        "            batch = X[i*batch_size:(i+1) * batch_size, :]\n",
        "            yield batch\n",
        "        \n",
        "        else:\n",
        "            batch = X[end: , :]\n",
        "            n += X[end:, :].shape[0]\n",
        "            yield batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqWJcIEehtNY"
      },
      "outputs": [],
      "source": [
        "def cnn_model(sequence_length, embedding_dim):\n",
        "    # Model Hyperparameters\n",
        "    n_classes = 2\n",
        "    filter_sizes = [1, 1, 1]\n",
        "    num_filters = 100\n",
        "    dropout_prob = [0.25, 0.5]\n",
        "\n",
        "    cnn_layer_in = Input(shape=(sequence_length, embedding_dim))\n",
        "    convs = []\n",
        "    for filter_size in filter_sizes:\n",
        "        conv = Convolution1D(nb_filter=num_filters,\n",
        "                             filter_length=filter_size,\n",
        "                             activation='relu')(cnn_layer_in)\n",
        "        pool = GlobalMaxPooling1D()(conv)\n",
        "        convs.append(pool)\n",
        "\n",
        "    out = Merge(mode='concat')(convs)\n",
        "    cnn_layer = Model(input=cnn_layer_in, output=out)\n",
        "\n",
        "    # Main sequential model\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Embedding(len(vocab) + 1, embedding_dim, input_length=sequence_length, trainable=None))\n",
        "    model.add(Dropout(dropout_prob[0]))\n",
        "    model.add(cnn_layer)\n",
        "    model.add(Dropout(dropout_prob[1]))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(n_classes))\n",
        "    model.add(Activation('softmax'))\n",
        "    model.compile(loss=LOSS_FUN, optimizer=OPTIMIZER, metrics=['accuracy'])\n",
        "    print(model.summary())\n",
        "    return model\n",
        "\n",
        "def lstm_model(sequence_length, embedding_dim):\n",
        "    n_classes = 2\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(vocab) + 1, embedding_dim, input_length=sequence_length, trainable=None))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(LSTM(50))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(n_classes, activation='softmax'))\n",
        "    model.compile(loss=LOSS_FUN, optimizer=OPTIMIZER, metrics=['accuracy'])\n",
        "\n",
        "    print(model.summary())\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2kirf-lXMAe"
      },
      "outputs": [],
      "source": [
        "def train_model(X, y, model, inp_dim, epochs=EPOCHS, batch_size=BATCH_SIZE):\n",
        "    folds = KFold(n_splits=NO_OF_FOLDS, shuffle=True, random_state=42)\n",
        "    print(folds)\n",
        "\n",
        "    tweet_length = X.shape[1]\n",
        "    total_loss, total_acc = [], []\n",
        "    \n",
        "    for train_index, test_index in folds.split(X):\n",
        "        shuffle_weights(model)\n",
        "\n",
        "        X_train, y_train = X[train_index], y[train_index]\n",
        "        X_test, y_test = X[test_index], y[test_index]\n",
        "\n",
        "        y_train = y_train.reshape((len(y_train), 1))\n",
        "        X_y_stacked = np.hstack((X_train, y_train))\n",
        "        for epoch in range(epochs):\n",
        "            print(f'EPOCH {epoch + 1}')\n",
        "            for batch in batch_gen(X_y_stacked, batch_size):\n",
        "                X_batch, y_batch = batch[:, :tweet_length], batch[:, tweet_length]\n",
        "                try:\n",
        "                    y_batch = np_utils.to_categorical(y_batch, nb_classes=2)\n",
        "                except Exception as e:\n",
        "                    print(e)\n",
        "\n",
        "                loss, acc = model.train_on_batch(X_batch, y_batch)\n",
        "                total_loss.append(loss)\n",
        "                total_acc.append(acc)\n",
        "\n",
        "        y_pred = model.predict_on_batch(X_test)\n",
        "        y_pred = np.argmax(y_pred, axis=1)\n",
        "        print(classification_report(y_test, y_pred))\n",
        "    \n",
        "    return total_loss, total_acc\n",
        "\n",
        "def plot_loss_acc(total_loss, total_acc):\n",
        "  # summarize history for accuracy and loss\n",
        "  plt.plot(total_loss)\n",
        "  plt.title('Training loss')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('batch')\n",
        "  plt.show()\n",
        "\n",
        "  plt.plot(total_acc)\n",
        "  plt.title('Training accuracy')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('batch')\n",
        "\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pXwu0X2t1_Y",
        "outputId": "b741e8d8-5196-4d67-b831-f2fa3f6f9e70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "41\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0 116  30 421 570  30 571 572 376  30 318 376\n",
            " 573  30   8 569   1]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "X, y = [], []\n",
        "for index, row in df.iterrows():\n",
        "    text = glove_tokenize(row['tweet'].lower())\n",
        "    text = ' '.join([c for c in text if c not in punctuation])\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in STOPWORDS]\n",
        "    seq, _emb = [], []\n",
        "    for word in words:\n",
        "        seq.append(vocab.get(word, vocab['UNK']))\n",
        "    X.append(seq)\n",
        "    y.append(row['label'])\n",
        "\n",
        "print(max([len(x) for x in X]))\n",
        "data = pad_sequences(X, maxlen=max([len(x) for x in X]))\n",
        "print(data[88])\n",
        "y = np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "YEE-cNUCopzC",
        "outputId": "5cd9ee7f-2f4a-4d32-df47-98fbb85e419c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "____________________________________________________________________________________________________\n",
            "Layer (type)                     Output Shape          Param #     Connected to                     \n",
            "====================================================================================================\n",
            "embedding_5 (Embedding)          (None, 41, 25)        1482275     embedding_input_5[0][0]          \n",
            "____________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)              (None, 41, 25)        0           embedding_5[0][0]                \n",
            "____________________________________________________________________________________________________\n",
            "model_5 (Model)                  (None, 300)           7800        dropout_9[0][0]                  \n",
            "____________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)             (None, 300)           0           model_5[1][0]                    \n",
            "____________________________________________________________________________________________________\n",
            "activation_9 (Activation)        (None, 300)           0           dropout_10[0][0]                 \n",
            "____________________________________________________________________________________________________\n",
            "dense_5 (Dense)                  (None, 2)             602         activation_9[0][0]               \n",
            "____________________________________________________________________________________________________\n",
            "activation_10 (Activation)       (None, 2)             0           dense_5[0][0]                    \n",
            "====================================================================================================\n",
            "Total params: 1,490,677\n",
            "Trainable params: 8,402\n",
            "Non-trainable params: 1,482,275\n",
            "____________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_c3be8f64-a1bc-40f1-a58b-ae033d67d685\", \"model_cnn.png\", 19511)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "data, y = sklearn.utils.shuffle(data, y)\n",
        "cnn = cnn_model(data.shape[1], EMBEDDING_DIM)\n",
        "\n",
        "# from keras.utils.visualize_util import plot\n",
        "# plot(cnn, to_file='model_cnn.png')\n",
        "# from google.colab import files\n",
        "# files.download('model_cnn.png') \n",
        "\n",
        "total_loss, total_acc = train_model(data, y, cnn, EMBEDDING_DIM)\n",
        "plot_loss_acc(total_loss, total_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "9fB31pi1Xdgg",
        "outputId": "349a7279-a217-4d17-b94f-8150d3ef768d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "____________________________________________________________________________________________________\n",
            "Layer (type)                     Output Shape          Param #     Connected to                     \n",
            "====================================================================================================\n",
            "embedding_6 (Embedding)          (None, 41, 25)        1482275     embedding_input_6[0][0]          \n",
            "____________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)             (None, 41, 25)        0           embedding_6[0][0]                \n",
            "____________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                    (None, 50)            15200       dropout_11[0][0]                 \n",
            "____________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)             (None, 50)            0           lstm_1[0][0]                     \n",
            "____________________________________________________________________________________________________\n",
            "dense_6 (Dense)                  (None, 2)             102         dropout_12[0][0]                 \n",
            "====================================================================================================\n",
            "Total params: 1,497,577\n",
            "Trainable params: 15,302\n",
            "Non-trainable params: 1,482,275\n",
            "____________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_c821225e-a7e6-426c-b6b8-1f7fce6e0180\", \"model_lstm.png\", 19511)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "data, y = sklearn.utils.shuffle(data, y)\n",
        "lstm = lstm_model(data.shape[1], EMBEDDING_DIM)\n",
        "\n",
        "# from keras.utils.visualize_util import plot\n",
        "# plot(cnn, to_file='model_lstm.png')\n",
        "# from google.colab import files\n",
        "# files.download('model_lstm.png')\n",
        "\n",
        "total_loss, total_acc = train_model(data, y, lstm, EMBEDDING_DIM)\n",
        "plot_loss_acc(total_loss, total_acc)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ML Proj - Hate speech detector (CNN)",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}